{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multi-Agent Dynamic Grid World Environment\n",
    "Created by: Ardianto Wibowo\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Add the path to the 'env' folder to sys.path\n",
    "sys.path.append('env')\n",
    "\n",
    "from ma_gridworld import Env\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, num_actions, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.num_actions = num_actions\n",
    "        self.q_table = {}  # Dictionary to store Q-values for each (x, y) coordinate\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select an action using epsilon-greedy policy.\"\"\"\n",
    "        # Initialize Q-values if state is not in the q_table\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = [0] * self.num_actions\n",
    "        \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.num_actions)  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        \"\"\"Update the Q-value for the given state-action pair.\"\"\"\n",
    "        # Initialize Q-values if state or next_state is not in the q_table\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = [0] * self.num_actions\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = [0] * self.num_actions\n",
    "        \n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        self.q_table[state][action] += self.alpha * (\n",
    "            reward + self.gamma * self.q_table[next_state][best_next_action] - self.q_table[state][action]\n",
    "        )\n",
    "\n",
    "    def decay_epsilon(self, epsilon_decay, epsilon_min):\n",
    "        \"\"\"Decay epsilon after each episode.\"\"\"\n",
    "        self.epsilon = max(self.epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "\n",
    "def get_action(agent_id, observation, num_actions, agents, env):\n",
    "    \"\"\"\n",
    "    This method provides an action chosen by each agent's Q-table:\n",
    "    1: up, 2: down, 3: left, 4: right, 0: stay\n",
    "    \"\"\"\n",
    "    coordinate_observation = tuple(observation[0])  # Keep observation as (x, y) tuple\n",
    "\n",
    "    #optional observation data may be used, depend on the agent needs.\n",
    "    win_state_observation = observation[1]\n",
    "    sensor_data_observation = observation[2]\n",
    "    comm_observation = observation[3]\n",
    "\n",
    "    physical_action = agents[agent_id].select_action(coordinate_observation)\n",
    "\n",
    "    if env.is_agent_silent:\n",
    "        comm_action = []  # No communication if agent is silent\n",
    "    else:\n",
    "        comm_action = np.random.choice(num_actions)  # Example random communication action\n",
    "\n",
    "    return physical_action, comm_action\n",
    "\n",
    "\n",
    "def run(num_episodes, max_steps_per_episode, agents, num_actions, env, epsilon_decay=0.95, epsilon_min=0.01):\n",
    "    for episode in range(num_episodes):\n",
    "        print(f\"Starting episode {episode + 1}\")\n",
    "        \n",
    "        observations = env.reset()  # Reset the environment at the start of each episode\n",
    "        done = [False] * env.num_agents\n",
    "        step_count = 0\n",
    "\n",
    "        while not all(done) and step_count < max_steps_per_episode:\n",
    "            actions = []\n",
    "            next_observations = []\n",
    "\n",
    "            for agent_id in range(env.num_agents):\n",
    "                observation = observations[agent_id]\n",
    "                action = get_action(agent_id, observation, num_actions, agents, env)\n",
    "                \n",
    "                actions.append(action)\n",
    "                next_observations.append(observation)\n",
    "\n",
    "            next_observations, rewards, done = env.step(actions)\n",
    "            step_count += 1\n",
    "\n",
    "            # Update Q-tables for each agent\n",
    "            for agent_id in range(env.num_agents):\n",
    "                observation = observations[agent_id]\n",
    "                coordinate_observation = tuple(observation[0])  # Use (x,y) tuple as observation data\n",
    "\n",
    "                reward = rewards[agent_id]\n",
    "\n",
    "                action = actions[agent_id]\n",
    "                physical_action = action[0]\n",
    "                \n",
    "                next_observation = next_observations[agent_id]\n",
    "                coordinate_next_observation = tuple(next_observation[0])  # Use (x,y) tuple as next observation data\n",
    "                \n",
    "                #optional next observation data may be used, depend on the agent needs.\n",
    "                win_state_next_observation = next_observation[1]\n",
    "                sensor_data_next_observation = next_observation[2]\n",
    "                comm_next_observation = next_observation[3]\n",
    "\n",
    "\n",
    "                agents[agent_id].update_q_table(coordinate_observation, physical_action, reward, coordinate_next_observation)\n",
    "\n",
    "            observations = next_observations\n",
    "            \n",
    "            # Render the environment\n",
    "            env.render()\n",
    "\n",
    "            print(f\"Step {step_count}:\")\n",
    "            for agent_id in range(env.num_agents):\n",
    "                print(f\"  Agent {agent_id}: Observation: {observations}, Action: {actions[agent_id]}, Reward: {rewards[agent_id]}, Done: {done[agent_id]}\")\n",
    "\n",
    "        print(f\"Episode {episode + 1} finished after {step_count} steps.\\n\")\n",
    "\n",
    "        # Decay epsilon for each agent\n",
    "        for agent in agents:\n",
    "            agent.decay_epsilon(epsilon_decay, epsilon_min)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    gsize=15 #grid size (square)\n",
    "    gpixels=30 #grid cell size in pixels\n",
    "\n",
    "    is_sensor_active = True #True:  Activate the sensory observation data\n",
    "    sensory_size = 3 #'is_sensor_active' must be True. The value must be odd, if event will be converted to one level odd number above\n",
    "    \n",
    "    num_agents = 3 #the number of agents will be run in paralel\n",
    "    num_obstacles = 10 #the number of obstacles\n",
    "    is_single_target = True #True: all agents have a single target, False: each agent has their own target\n",
    "    num_targets_per_agent = 2 #'is_single_target' must be true to have an effect\n",
    "    \n",
    "    is_agent_silent = True #True: communication among agents is allowed\n",
    "\n",
    "    num_episodes=150 #the number of episode will be run\n",
    "    max_steps_per_episode=400 #each episode will be stopped when max_step is reached\n",
    "\n",
    "    eps_moving_targets = 151 #set this value greater than 'num_episodes' to keep the targets in a stationary position\n",
    "    eps_moving_obstacles = 151 #set this value greater than 'num_episodes' to keep the obstacles in a stationary position\n",
    "\n",
    "    render = True #True: render the animation into the screen (so far, it is still can not be deactivated)\n",
    "\n",
    "    min_obstacle_distance_from_target = 1 #min grid distance of each obstacles relative to targets\n",
    "    max_obstacle_distance_from_target = 5 #max grid distance of each obstacles relative to targets\n",
    "    min_obstacle_distance_from_agents = 1 #min grid distance of each obstacles relative to agents\n",
    "\n",
    "    reward_normal = -1 #reward value of normal steps\n",
    "    reward_obstacle = -5 #reward value when hit an obstacle\n",
    "    reward_target = 50 #reward value when reach the target\n",
    "\n",
    "    is_totally_random = False #True: target and obstacles initial as well as movement position is always random on each call, False: only random at the beginning. \n",
    "    animation_speed = 0.0000001 #smaller is faster \n",
    "    is_destroy_environment = True #True: automatically close the animation after all episodes end.  \n",
    "\n",
    "    # Initialize environment\n",
    "    env = Env(\n",
    "        num_agents=num_agents, num_targets_per_agent=num_targets_per_agent, num_obstacles=num_obstacles,\n",
    "        eps_moving_obstacles=eps_moving_obstacles, eps_moving_targets=eps_moving_targets,\n",
    "        is_agent_silent=is_agent_silent, is_single_target=is_single_target, sensory_size=sensory_size,\n",
    "        gpixels=gpixels, gheight=gsize, gwidth=gsize, is_sensor_active=is_sensor_active,\n",
    "        min_obstacle_distance_from_target=min_obstacle_distance_from_target,\n",
    "        max_obstacle_distance_from_target=max_obstacle_distance_from_target,\n",
    "        min_obstacle_distance_from_agents=min_obstacle_distance_from_agents,\n",
    "        is_totally_random=is_totally_random, animation_speed=animation_speed,\n",
    "        reward_normal=reward_normal, reward_obstacle=reward_obstacle, reward_target=reward_target\n",
    "    )\n",
    "\n",
    "    alpha=0.1\n",
    "    gamma=0.9\n",
    "    epsilon=0.1\n",
    "    epsilon_decay = 0.95 \n",
    "    epsilon_min = 0.01\n",
    "    num_actions = len(env.action_space)\n",
    "\n",
    "    # Initialize Q-learning agents\n",
    "    agents = [QLearningAgent(num_actions, alpha, gamma, epsilon) for _ in range(num_agents)]\n",
    "\n",
    "    # Run episodes\n",
    "    run(num_episodes, max_steps_per_episode, agents, num_actions, env, epsilon_decay, epsilon_min)\n",
    "\n",
    "    # Destroy environment if needed\n",
    "    if is_destroy_environment:\n",
    "        env.destroy_environment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
