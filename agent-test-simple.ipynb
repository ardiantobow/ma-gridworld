{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multi-Agent Dynamic Grid World Environment\n",
    "Created by: Ardianto Wibowo\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Add the path to the 'env' folder to sys.path\n",
    "sys.path.append('env')\n",
    "\n",
    "from ma_gridworld import Env\n",
    "\n",
    "\n",
    "def get_action(num_actions):\n",
    "    \"\"\"\n",
    "    This method provide a random action chosen recognized by the ma-gridworld environment:\n",
    "    1: up, 2: down, 3: left, 4: right, 0: stay\n",
    "    \"\"\"\n",
    "    physical_action = np.random.choice(num_actions) # example of random value as a physical action\n",
    "\n",
    "    if env.is_agent_silent:\n",
    "        comm_action = [] # communication action is set to be zero if agent silent\n",
    "    else:\n",
    "        comm_action = np.random.choice(num_actions) # example of random value as a communication action\n",
    "    \n",
    "    return (physical_action, comm_action)\n",
    "\n",
    "\n",
    "def run(num_episodes, max_steps_per_episode):\n",
    "    for episode in range(num_episodes):\n",
    "        print(f\"Starting episode {episode + 1}\")\n",
    "        observations = env.reset()  # Reset the environment at the start of each episode\n",
    "        \n",
    "        done = [False] * env.num_agents  # Initialize 'done' as a list for each agent\n",
    "        step_count = 0\n",
    "\n",
    "        while not all(done) and step_count < max_steps_per_episode:  # Stop if all agents are done or max steps reached\n",
    "            actions = []\n",
    "            next_observations = []\n",
    "            \n",
    "            for agent_id in range(env.num_agents):\n",
    "                action = get_action(num_actions)  # Call get action method to determine an action\n",
    "                actions.append(action)\n",
    "\n",
    "            next_observations, rewards, done = env.step(actions)  # Step in the environment\n",
    "            \n",
    "            \n",
    "            observations = next_observations\n",
    "            step_count += 1\n",
    "\n",
    "            # Render the environment\n",
    "            env.render()\n",
    "\n",
    "            print(f\"Step {step_count}:\")\n",
    "            for agent_id in range(env.num_agents):\n",
    "                print(f\"  Agent {agent_id}: Observation: {observations}, Action: {actions[agent_id]}, Reward: {rewards[agent_id]}, Done: {done[agent_id]}\")\n",
    "\n",
    "        \n",
    "        print(f\"Episode {episode + 1} finished after {step_count} steps.\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    gsize=15 #grid size (square)\n",
    "    gpixels=30 #grid cell size in pixels\n",
    "\n",
    "    is_sensor_active = True #True:  Activate the sensory observation data\n",
    "    sensory_size = 3 #'is_sensor_active' must be True. The value must be odd, if event will be converted to one level odd number above\n",
    "    \n",
    "    num_agents = 3 #the number of agents will be run in paralel\n",
    "    num_obstacles = 10 #the number of obstacles\n",
    "    is_single_target = True #True: all agents have a single target, False: each agent has their own target\n",
    "    num_targets_per_agent = 2 #'is_single_target' must be true to have an effect\n",
    "    \n",
    "    is_agent_silent = True #True: communication among agents is allowed\n",
    "\n",
    "    num_episodes=150 #the number of episode will be run\n",
    "    max_steps_per_episode=400 #each episode will be stopped when max_step is reached\n",
    "\n",
    "    eps_moving_targets = 151 #set this value greater than 'num_episodes' to keep the targets in a stationary position\n",
    "    eps_moving_obstacles = 151 #set this value greater than 'num_episodes' to keep the obstacles in a stationary position\n",
    "\n",
    "    render = True #True: render the animation into the screen (so far, it is still can not be deactivated)\n",
    "\n",
    "    min_obstacle_distance_from_target = 1 #min grid distance of each obstacles relative to targets\n",
    "    max_obstacle_distance_from_target = 5 #max grid distance of each obstacles relative to targets\n",
    "    min_obstacle_distance_from_agents = 1 #min grid distance of each obstacles relative to agents\n",
    "\n",
    "    reward_normal = -1 #reward value of normal steps\n",
    "    reward_obstacle = -5 #reward value when hit an obstacle\n",
    "    reward_target = 50 #reward value when reach the target\n",
    "\n",
    "    is_totally_random = False #True: target and obstacles initial as well as movement position is always random on each call, False: only random at the beginning. \n",
    "    animation_speed = 0.0000001 #smaller is faster \n",
    "    is_destroy_environment = True #True: automatically close the animation after all episodes end.  \n",
    "\n",
    "    # Initialize environment\n",
    "    env = Env(\n",
    "        num_agents=num_agents, num_targets_per_agent=num_targets_per_agent, num_obstacles=num_obstacles,\n",
    "        eps_moving_obstacles=eps_moving_obstacles, eps_moving_targets=eps_moving_targets,\n",
    "        is_agent_silent=is_agent_silent, is_single_target=is_single_target, sensory_size=sensory_size,\n",
    "        gpixels=gpixels, gheight=gsize, gwidth=gsize, is_sensor_active=is_sensor_active,\n",
    "        min_obstacle_distance_from_target=min_obstacle_distance_from_target,\n",
    "        max_obstacle_distance_from_target=max_obstacle_distance_from_target,\n",
    "        min_obstacle_distance_from_agents=min_obstacle_distance_from_agents,\n",
    "        is_totally_random=is_totally_random, animation_speed=animation_speed,\n",
    "        reward_normal=reward_normal, reward_obstacle=reward_obstacle, reward_target=reward_target\n",
    "    )\n",
    "    \n",
    "    num_actions = len(env.action_space)\n",
    "    \n",
    "    agent = run(num_episodes, max_steps_per_episode)\n",
    "\n",
    "    if is_destroy_environment:\n",
    "        env.destroy_environment() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
